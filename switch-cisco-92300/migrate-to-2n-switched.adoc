---
permalink: switch-cisco-92300/migrate-to-2n-switched.html 
sidebar: sidebar 
keywords: 2-node switched Cisco 92300YC cluster switch migration 
summary: 'Si vous disposez déjà d"un environnement de cluster sans commutateur à deux nœuds, vous pouvez migrer vers un environnement de cluster avec commutateur à deux nœuds à l"aide des commutateurs Cisco Nexus 92300YC. Vous pouvez ainsi évoluer au-delà de deux nœuds du cluster.' 
---
= Migrez vers un cluster à deux nœuds avec commutateur Cisco Nexus 92300YC
:allow-uri-read: 
:icons: font
:imagesdir: ../media/


[role="lead"]
Si vous disposez déjà d'un environnement de cluster à deux nœuds _sans commutateur_, vous pouvez migrer vers un environnement de cluster à deux nœuds _commuté_ à l'aide de commutateurs Cisco Nexus 92300YC qui vous permettent d'évoluer au-delà de deux nœuds du cluster.

La procédure que vous utilisez dépend de votre présence de deux ports cluster-network dédiés sur chaque contrôleur ou d'un port de cluster unique sur chaque contrôleur. Le processus documenté fonctionne pour tous les nœuds utilisant des ports optiques ou twinax, mais n'est pas pris en charge sur ce commutateur si les nœuds utilisent des ports RJ45 10 Gb BASE-T intégrés pour les ports du réseau en cluster.

La plupart des systèmes requièrent deux ports dédiés du réseau cluster sur chaque contrôleur.


NOTE: Une fois la migration terminée, vous devrez peut-être installer le fichier de configuration requis pour prendre en charge le Cluster Switch Health Monitor (CSHM) pour les commutateurs de cluster 92300YC. Voirlink:setup-install-cshm-file.html["Installer le moniteur d'intégrité du commutateur de cluster (CSHM)"].



== Examen des conditions requises

.Ce dont vous avez besoin
Dans le cas d'une configuration sans commutateur à 2 nœuds, vérifiez les points suivants :

* La configuration sans commutateur à 2 nœuds est correctement configurée et opérationnelle.
* Les nœuds exécutent ONTAP 9.6 et version ultérieure.
* Tous les ports de cluster sont à l'état *up*.
* Toutes les interfaces logiques (LIF) de cluster sont à l'état *up* et sur leurs ports de base.


Pour la configuration du commutateur Cisco Nexus 92300YC :

* Les deux commutateurs disposent d'une connectivité réseau de gestion.
* Il y a un accès à la console aux commutateurs du cluster.
* Les switchs nœud à nœud Nexus 92300YC et les connexions switch à commutateur utilisent des câbles Twinax ou à fibre optique.
+
https://["Hardware Universe - commutateurs"^] contient plus d'informations sur le câblage.

* Les câbles ISL (Inter-Switch Link) sont connectés aux ports 1/65 et 1/66 des deux commutateurs 92300YC.
* La personnalisation initiale des deux commutateurs 92300YC est terminée. Pour que :
+
** Les commutateurs 92300YC exécutent la dernière version du logiciel
** Les fichiers RCF (Reference Configuration File) sont appliqués aux switchs toute personnalisation de site, telle que SMTP, SNMP et SSH, est configurée sur les nouveaux switchs.






== Migrer le commutateur

.À propos des exemples
Les exemples de cette procédure utilisent la nomenclature de commutateurs et de nœuds du cluster suivante :

* Les noms des commutateurs 92300YC sont cs1 et cs2.
* Les noms des SVM du cluster sont les nœuds 1 et nœud2.
* Les noms des LIFs sont respectivement node1_clude1 et node1_clus2 sur le nœud 1, et node2_clude1 et node2_clus2 sur le nœud 2.
* Le `cluster1::*>` l'invite indique le nom du cluster.
* les ports de cluster utilisés dans cette procédure sont e0a et e0b.
+
https://["Hardware Universe"^] contient les informations les plus récentes sur les ports de cluster réels de vos plates-formes.





=== Étape 1 : préparer la migration

. Modifiez le niveau de privilège en avancé, en saisissant `y` lorsque vous êtes invité à continuer :
+
`set -privilege advanced`

+
L'invite avancée (`*>`) s'affiche.

. Si AutoSupport est activé sur ce cluster, supprimez la création automatique de dossiers en invoquant un message AutoSupport :
+
`system node autosupport invoke -node * -type all -message MAINT=xh`

+
où x représente la durée de la fenêtre de maintenance en heures.

+

NOTE: Le message AutoSupport informe le support technique de cette tâche de maintenance de sorte que la création automatique de dossier soit supprimée lors de la fenêtre de maintenance.

+
.Montrer l'exemple
[%collapsible]
====
La commande suivante supprime la création automatique de dossiers pendant deux heures :

[listing, subs="+quotes"]
----
cluster1::*> *system node autosupport invoke -node * -type all -message MAINT=2h*
----
====




=== Étape 2 : configurer les câbles et les ports

. Désactivez tous les ports orientés nœuds (et non les ports ISL) sur les nouveaux commutateurs de cluster cs1 et cs2.
+
Vous ne devez pas désactiver les ports ISL.

+
.Montrer l'exemple
[%collapsible]
====
L'exemple suivant montre que les ports 1 à 64 orientés nœud sont désactivés sur le commutateur cs1 :

[listing, subs="+quotes"]
----
cs1# *config*
Enter configuration commands, one per line. End with CNTL/Z.
cs1(config)# *interface e/1-64*
cs1(config-if-range)# *shutdown*
----
====
. Vérifiez que l'ISL et les ports physiques de l'ISL entre les deux commutateurs 92300YC cs1 et cs2 sont en service sur les ports 1/65 et 1/66 :
+
`show port-channel summary`

+
.Montrer l'exemple
[%collapsible]
====
L'exemple suivant montre que les ports ISL sont active sur le commutateur cs1 :

[listing, subs="+quotes"]
----
cs1# *show port-channel summary*

Flags:  D - Down        P - Up in port-channel (members)
        I - Individual  H - Hot-standby (LACP only)
        s - Suspended   r - Module-removed
        b - BFD Session Wait
        S - Switched    R - Routed
        U - Up (port-channel)
        p - Up in delay-lacp mode (member)
        M - Not in use. Min-links not met
--------------------------------------------------------------------------------
Group Port-       Type     Protocol  Member Ports
      Channel
--------------------------------------------------------------------------------
1     Po1(SU)     Eth      LACP      Eth1/65(P)   Eth1/66(P)
----
+ l'exemple suivant montre que les ports ISL sont activés sur le commutateur cs2 :

+

[listing, subs="+quotes"]
----
(cs2)# *show port-channel summary*

Flags:  D - Down        P - Up in port-channel (members)
        I - Individual  H - Hot-standby (LACP only)
        s - Suspended   r - Module-removed
        b - BFD Session Wait
        S - Switched    R - Routed
        U - Up (port-channel)
        p - Up in delay-lacp mode (member)
        M - Not in use. Min-links not met
--------------------------------------------------------------------------------
Group Port-       Type     Protocol  Member Ports
      Channel
--------------------------------------------------------------------------------
1     Po1(SU)     Eth      LACP      Eth1/65(P)   Eth1/66(P)
----
====
. Afficher la liste des périphériques voisins :
+
`show cdp neighbors`

+
Cette commande fournit des informations sur les périphériques connectés au système.

+
.Montrer l'exemple
[%collapsible]
====
L'exemple suivant répertorie les périphériques voisins sur le commutateur cs1 :

[listing, subs="+quotes"]
----
cs1# *show cdp neighbors*

Capability Codes: R - Router, T - Trans-Bridge, B - Source-Route-Bridge
                  S - Switch, H - Host, I - IGMP, r - Repeater,
                  V - VoIP-Phone, D - Remotely-Managed-Device,
                  s - Supports-STP-Dispute

Device-ID          Local Intrfce  Hldtme Capability  Platform      Port ID
cs2(FDO220329V5)    Eth1/65        175    R S I s   N9K-C92300YC  Eth1/65
cs2(FDO220329V5)    Eth1/66        175    R S I s   N9K-C92300YC  Eth1/66

Total entries displayed: 2
----
+ l'exemple suivant répertorie les périphériques voisins sur le commutateur cs2 :

+

[listing, subs="+quotes"]
----
cs2# *show cdp neighbors*

Capability Codes: R - Router, T - Trans-Bridge, B - Source-Route-Bridge
                  S - Switch, H - Host, I - IGMP, r - Repeater,
                  V - VoIP-Phone, D - Remotely-Managed-Device,
                  s - Supports-STP-Dispute

Device-ID          Local Intrfce  Hldtme Capability  Platform      Port ID
cs1(FDO220329KU)    Eth1/65        177    R S I s   N9K-C92300YC  Eth1/65
cs1(FDO220329KU)    Eth1/66        177    R S I s   N9K-C92300YC  Eth1/66

Total entries displayed: 2
----
====
. Vérifier que tous les ports du cluster sont bien :
+
`network port show -ipspace Cluster`

+
Chaque port doit s'afficher pendant `Link` et en bonne santé pour `Health Status`.

+
.Montrer l'exemple
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network port show -ipspace Cluster*

Node: node1

                                                  Speed(Mbps) Health
Port      IPspace      Broadcast Domain Link MTU  Admin/Oper  Status
--------- ------------ ---------------- ---- ---- ----------- --------
e0a       Cluster      Cluster          up   9000  auto/10000 healthy
e0b       Cluster      Cluster          up   9000  auto/10000 healthy

Node: node2

                                                  Speed(Mbps) Health
Port      IPspace      Broadcast Domain Link MTU  Admin/Oper  Status
--------- ------------ ---------------- ---- ---- ----------- --------
e0a       Cluster      Cluster          up   9000  auto/10000 healthy
e0b       Cluster      Cluster          up   9000  auto/10000 healthy

4 entries were displayed.
----
====
. Vérifier que toutes les LIFs de cluster sont opérationnelles :
+
`network interface show -vserver Cluster`

+
Chaque LIF de cluster doit afficher la valeur true pour `Is Home` et avoir un `Status Admin/Oper` de haut/haut

+
.Montrer l'exemple
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network interface show -vserver Cluster*

            Logical    Status     Network            Current       Current Is
Vserver     Interface  Admin/Oper Address/Mask       Node          Port    Home
----------- ---------- ---------- ------------------ ------------- ------- -----
Cluster
            node1_clus1  up/up    169.254.209.69/16  node1         e0a     true
            node1_clus2  up/up    169.254.49.125/16  node1         e0b     true
            node2_clus1  up/up    169.254.47.194/16  node2         e0a     true
            node2_clus2  up/up    169.254.19.183/16  node2         e0b     true
4 entries were displayed.
----
====
. Vérifiez que la fonction de restauration automatique est activée sur l'ensemble des LIFs du cluster :
+
`network interface show -vserver Cluster -fields auto-revert`

+
.Montrer l'exemple
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network interface show -vserver Cluster -fields auto-revert*

          Logical
Vserver   Interface     Auto-revert
--------- ------------- ------------
Cluster
          node1_clus1   true
          node1_clus2   true
          node2_clus1   true
          node2_clus2   true

4 entries were displayed.
----
====
. Débranchez le câble du port du cluster e0a sur le nœud 1, puis connectez e0a au port 1 du commutateur cs1 du cluster, à l'aide du câblage approprié pris en charge par les commutateurs 92300YC.
+
Le https://["_Hardware Universe - interrupteurs_"^] contient plus d'informations sur le câblage.

. Débranchez le câble du port du cluster e0a sur le nœud 2, puis connectez e0a au port 2 du commutateur cs1 du cluster, à l'aide du câblage approprié pris en charge par les commutateurs 92300YC.
. Activer tous les ports orientés nœuds sur le commutateur de cluster cs1.
+
.Montrer l'exemple
[%collapsible]
====
L'exemple suivant montre que les ports 1/1 à 1/64 sont activés sur le commutateur cs1 :

[listing, subs="+quotes"]
----
cs1# *config*
Enter configuration commands, one per line. End with CNTL/Z.
cs1(config)# *interface e1/1-64*
cs1(config-if-range)# *no shutdown*
----
====
. Vérifier que toutes les LIFs du cluster sont opérationnelles et affichées comme true pour `Is Home`:
+
`network interface show -vserver Cluster`

+
.Montrer l'exemple
[%collapsible]
====
L'exemple suivant montre que toutes les LIFs sont up sur le nœud1 et le nœud2, ainsi `Is Home` les résultats sont vrais :

[listing, subs="+quotes"]
----
cluster1::*> *network interface show -vserver Cluster*

         Logical      Status     Network            Current     Current Is
Vserver  Interface    Admin/Oper Address/Mask       Node        Port    Home
-------- ------------ ---------- ------------------ ----------- ------- ----
Cluster
         node1_clus1  up/up      169.254.209.69/16  node1       e0a     true
         node1_clus2  up/up      169.254.49.125/16  node1       e0b     true
         node2_clus1  up/up      169.254.47.194/16  node2       e0a     true
         node2_clus2  up/up      169.254.19.183/16  node2       e0b     true

4 entries were displayed.
----
====
. Afficher des informations relatives à l'état des nœuds du cluster :
+
`cluster show`

+
.Montrer l'exemple
[%collapsible]
====
L'exemple suivant affiche des informations sur la santé et l'éligibilité des nœuds du cluster :

[listing, subs="+quotes"]
----
cluster1::*> *cluster show*

Node                 Health  Eligibility   Epsilon
-------------------- ------- ------------  ------------
node1                true    true          false
node2                true    true          false

2 entries were displayed.
----
====
. Déconnectez le câble du port de cluster e0b sur le nœud 1, puis connectez le port 1 du commutateur de cluster cs2 à l'aide du câblage approprié pris en charge par les commutateurs 92300YC.
. Déconnectez le câble du port de cluster e0b sur le nœud 2, puis connectez le port e0b au port 2 du commutateur de cluster cs2, en utilisant le câblage approprié pris en charge par les commutateurs 92300YC.
. Activer tous les ports orientés nœud sur le commutateur de cluster cs2.
+
.Montrer l'exemple
[%collapsible]
====
L'exemple suivant montre que les ports 1/1 à 1/64 sont activés sur le commutateur cs2 :

[listing, subs="+quotes"]
----
cs2# *config*
Enter configuration commands, one per line. End with CNTL/Z.
cs2(config)# *interface e1/1-64*
cs2(config-if-range)# *no shutdown*
----
====




=== Étape 3 : vérifier la configuration

. Vérifier que tous les ports du cluster sont bien :
+
`network port show -ipspace Cluster`

+
.Montrer l'exemple
[%collapsible]
====
L'exemple suivant montre que tous les ports du cluster apparaissent sur les nœuds 1 et sur le nœud 2 :

[listing, subs="+quotes"]
----
cluster1::*> *network port show -ipspace Cluster*

Node: node1
                                                                       Ignore
                                                  Speed(Mbps) Health   Health
Port      IPspace      Broadcast Domain Link MTU  Admin/Oper  Status   Status
--------- ------------ ---------------- ---- ---- ----------- -------- ------
e0a       Cluster      Cluster          up   9000  auto/10000 healthy  false
e0b       Cluster      Cluster          up   9000  auto/10000 healthy  false

Node: node2
                                                                       Ignore
                                                  Speed(Mbps) Health   Health
Port      IPspace      Broadcast Domain Link MTU  Admin/Oper  Status   Status
--------- ------------ ---------------- ---- ---- ----------- -------- ------
e0a       Cluster      Cluster          up   9000  auto/10000 healthy  false
e0b       Cluster      Cluster          up   9000  auto/10000 healthy  false

4 entries were displayed.
----
====
. Vérifiez que toutes les interfaces affichent la valeur true pour `Is Home`:
+
`network interface show -vserver Cluster`

+

NOTE: Cette opération peut prendre plusieurs minutes.

+
.Montrer l'exemple
[%collapsible]
====
L'exemple suivant montre que toutes les LIFs sont up sur le nœud1 et celui du nœud2, ainsi que celui-ci `Is Home` les résultats sont vrais :

[listing, subs="+quotes"]
----
cluster1::*> *network interface show -vserver Cluster*

          Logical      Status     Network            Current    Current Is
Vserver   Interface    Admin/Oper Address/Mask       Node       Port    Home
--------- ------------ ---------- ------------------ ---------- ------- ----
Cluster
          node1_clus1  up/up      169.254.209.69/16  node1      e0a     true
          node1_clus2  up/up      169.254.49.125/16  node1      e0b     true
          node2_clus1  up/up      169.254.47.194/16  node2      e0a     true
          node2_clus2  up/up      169.254.19.183/16  node2      e0b     true

4 entries were displayed.
----
====
. Vérifier que les deux nœuds disposent chacun d'une connexion à chaque commutateur :
+
`show cdp neighbors`

+
.Montrer l'exemple
[%collapsible]
====
L'exemple suivant montre les résultats appropriés pour les deux commutateurs :

[listing, subs="+quotes"]
----
(cs1)# *show cdp neighbors*

Capability Codes: R - Router, T - Trans-Bridge, B - Source-Route-Bridge
                  S - Switch, H - Host, I - IGMP, r - Repeater,
                  V - VoIP-Phone, D - Remotely-Managed-Device,
                  s - Supports-STP-Dispute

Device-ID          Local Intrfce  Hldtme Capability  Platform      Port ID
node1               Eth1/1         133    H         FAS2980       e0a
node2               Eth1/2         133    H         FAS2980       e0a
cs2(FDO220329V5)    Eth1/65        175    R S I s   N9K-C92300YC  Eth1/65
cs2(FDO220329V5)    Eth1/66        175    R S I s   N9K-C92300YC  Eth1/66

Total entries displayed: 4


(cs2)# *show cdp neighbors*

Capability Codes: R - Router, T - Trans-Bridge, B - Source-Route-Bridge
                  S - Switch, H - Host, I - IGMP, r - Repeater,
                  V - VoIP-Phone, D - Remotely-Managed-Device,
                  s - Supports-STP-Dispute

Device-ID          Local Intrfce  Hldtme Capability  Platform      Port ID
node1               Eth1/1         133    H         FAS2980       e0b
node2               Eth1/2         133    H         FAS2980       e0b
cs1(FDO220329KU)
                    Eth1/65        175    R S I s   N9K-C92300YC  Eth1/65
cs1(FDO220329KU)
                    Eth1/66        175    R S I s   N9K-C92300YC  Eth1/66

Total entries displayed: 4
----
====
. Affiche des informations sur les périphériques réseau détectés dans votre cluster :
+
`network device-discovery show -protocol cdp`

+
.Montrer l'exemple
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network device-discovery show -protocol cdp*
Node/       Local  Discovered
Protocol    Port   Device (LLDP: ChassisID)  Interface         Platform
----------- ------ ------------------------- ----------------  ----------------
node2      /cdp
            e0a    cs1                       0/2               N9K-C92300YC
            e0b    cs2                       0/2               N9K-C92300YC
node1      /cdp
            e0a    cs1                       0/1               N9K-C92300YC
            e0b    cs2                       0/1               N9K-C92300YC

4 entries were displayed.
----
====
. Vérifiez que les paramètres sont désactivés :
+
`network options switchless-cluster show`

+

NOTE: La commande peut prendre plusieurs minutes. Attendez que l'annonce « 3 minutes d'expiration de la durée de vie » soit annoncée.

+
.Montrer l'exemple
[%collapsible]
====
La sortie FALSE dans l'exemple suivant montre que les paramètres de configuration sont désactivés :

[listing, subs="+quotes"]
----
cluster1::*> *network options switchless-cluster show*
Enable Switchless Cluster: false
----
====
. Vérifiez l'état des membres du nœud sur le cluster :
+
`cluster show`

+
.Montrer l'exemple
[%collapsible]
====
L'exemple suivant affiche des informations sur la santé et l'éligibilité des nœuds du cluster :

[listing, subs="+quotes"]
----
cluster1::*> *cluster show*

Node                 Health  Eligibility   Epsilon
-------------------- ------- ------------  --------
node1                true    true          false
node2                true    true          false
----
====
. Vérifiez que le réseau de cluster dispose d'une connectivité complète :
+
`cluster ping-cluster -node node-name`

+
.Montrer l'exemple
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::> *cluster ping-cluster -node node2*
Host is node2
Getting addresses from network interface table...
Cluster node1_clus1 169.254.209.69 node1 e0a
Cluster node1_clus2 169.254.49.125 node1 e0b
Cluster node2_clus1 169.254.47.194 node2 e0a
Cluster node2_clus2 169.254.19.183 node2 e0b
Local = 169.254.47.194 169.254.19.183
Remote = 169.254.209.69 169.254.49.125
Cluster Vserver Id = 4294967293
Ping status:

Basic connectivity succeeds on 4 path(s)
Basic connectivity fails on 0 path(s)

Detected 9000 byte MTU on 4 path(s):
Local 169.254.47.194 to Remote 169.254.209.69
Local 169.254.47.194 to Remote 169.254.49.125
Local 169.254.19.183 to Remote 169.254.209.69
Local 169.254.19.183 to Remote 169.254.49.125
Larger than PMTU communication succeeds on 4 path(s)
RPC status:
2 paths up, 0 paths down (tcp check)
2 paths up, 0 paths down (udp check)
----
====
. Si vous avez supprimé la création automatique de cas, réactivez-la en appelant un message AutoSupport :
+
`system node autosupport invoke -node * -type all -message MAINT=END`

+
.Montrer l'exemple
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *system node autosupport invoke -node * -type all -message MAINT=END*
----
====
. Rétablissez le niveau de privilège sur admin :
+
`set -privilege admin`

. Pour ONTAP 9.4 et versions ultérieures, activez la fonctionnalité de collecte des journaux du contrôle de l'état du commutateur de cluster pour collecter les fichiers journaux des commutateurs, à l'aide des commandes :
+
`system cluster-switch log setup-password` et `system cluster-switch log enable-collection`

+
.Montrer l'exemple
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *system cluster-switch log setup-password*
Enter the switch name: <return>
The switch name entered is not recognized.
Choose from the following list:
cs1
cs2

cluster1::*> *system cluster-switch log setup-password*

Enter the switch name: *cs1*
RSA key fingerprint is e5:8b:c6:dc:e2:18:18:09:36:63:d9:63:dd:03:d9:cc
Do you want to continue? {y|n}::[n] *y*

Enter the password: <enter switch password>
Enter the password again: <enter switch password>

cluster1::*> *system cluster-switch log setup-password*

Enter the switch name: *cs2*
RSA key fingerprint is 57:49:86:a1:b9:80:6a:61:9a:86:8e:3c:e3:b7:1f:b1
Do you want to continue? {y|n}:: [n] *y*

Enter the password: <enter switch password>
Enter the password again: <enter switch password>

cluster1::*> *system cluster-switch log enable-collection*

Do you want to enable cluster log collection for all nodes in the cluster?
{y|n}: [n] *y*

Enabling cluster switch log collection.

cluster1::*>
----
====
+

NOTE: Si l'une de ces commandes renvoie une erreur, contactez le support NetApp.


